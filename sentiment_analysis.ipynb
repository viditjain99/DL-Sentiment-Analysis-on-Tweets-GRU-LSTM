{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLassgn2q2_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvsquRftL85K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXWeAv3JBT3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My\\ Drive/DL/assgn2\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-fCg5Z9ANv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pickle\n",
        "import re\n",
        "import random\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "random.seed(2020)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQeqvKjiB6FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_tweets(tweets_int,seq_length):\n",
        "  features=np.zeros((len(tweets_int),seq_length),dtype=int)\n",
        "  for i,tweet in enumerate(tweets_int):\n",
        "    tweet_len=len(tweet)\n",
        "    if tweet_len<=seq_length:\n",
        "      zeros=list(np.zeros(seq_length-tweet_len))\n",
        "      padded_tweet=tweet+zeros\n",
        "    \n",
        "    features[i,:]=np.array(padded_tweet)\n",
        "  return features\n",
        "\n",
        "def train(model,iterator,optimiser,criterion):\n",
        "  epoch_loss=0\n",
        "  epoch_accuracy=0\n",
        "  model.train()\n",
        "  for i,batch in enumerate(iterator):\n",
        "    x=batch[0]\n",
        "    y=batch[1]\n",
        "    optimiser.zero_grad()\n",
        "    pred=model(x)\n",
        "    y=y.cpu().detach().numpy()\n",
        "    y=np.eye(13)[y]\n",
        "    y=torch.from_numpy(y).float().cuda()\n",
        "    loss=criterion(pred,y)\n",
        "    acc=accuracy(pred,y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    epoch_loss+=loss.item()\n",
        "    epoch_accuracy+=acc\n",
        "  return epoch_loss/len(iterator),epoch_accuracy/len(iterator)\n",
        "\n",
        "def evaluate(model,iterator,criterion):\n",
        "  model.eval()\n",
        "  epoch_loss=0\n",
        "  epoch_accuracy=0\n",
        "  with torch.no_grad():\n",
        "    for i,batch in enumerate(iterator):\n",
        "      x=batch[0]\n",
        "      y=batch[1]\n",
        "      pred=model(x)\n",
        "      y=y.cpu().detach().numpy()\n",
        "      y=np.eye(13)[y]\n",
        "      y=torch.from_numpy(y).float().cuda()\n",
        "      loss=criterion(pred,y)\n",
        "      acc=accuracy(pred,y)\n",
        "      epoch_loss+=loss.item()\n",
        "      epoch_accuracy+=acc\n",
        "  return epoch_loss/len(iterator),epoch_accuracy/len(iterator)\n",
        "\n",
        "def accuracy(pred,y):\n",
        "  count=0\n",
        "  for i in range(len(pred)):\n",
        "    idx=torch.argmax(pred[i])\n",
        "    idx_class=torch.argmax(y[i])\n",
        "    if idx.item()==idx_class.item():\n",
        "      count+=1\n",
        "  return count/len(y)\n",
        "\n",
        "def roc(gt,pred):\n",
        "  classes=list(set(gt))\n",
        "  gt=label_binarize(gt,classes)\n",
        "  pred=label_binarize(pred,classes)\n",
        "  plt.figure()\n",
        "  fpr=dict()\n",
        "  tpr=dict()\n",
        "  roc_auc=dict()\n",
        "  n_classes=len(classes)\n",
        "  mean_fpr=np.linspace(0,1,100)\n",
        "  tprs=[]\n",
        "  aucs=[]\n",
        "  for i in range(n_classes):\n",
        "    fpr[i],tpr[i],_=roc_curve(gt[:,i],pred[:,i])\n",
        "    roc_auc[i]=auc(fpr[i],tpr[i])\n",
        "    aucs.append(roc_auc[i])\n",
        "    tprs.append(np.interp(mean_fpr,fpr[i],tpr[i]))\n",
        "    tprs[-1][0]=0.0\n",
        "    plt.plot(fpr[i],tpr[i],lw=1,alpha=0.3)\n",
        "  mean_tpr=np.mean(tprs,axis=0)\n",
        "  mean_tpr[-1]=1.0\n",
        "  mean_auc=auc(mean_fpr,mean_tpr)\n",
        "  std_auc=np.std(aucs)\n",
        "  plt.plot(mean_fpr,mean_tpr,color='b',label=r'Mean ROC (AUC=%0.2f $\\pm$ %0.2f)'%(mean_auc,std_auc),lw=2,alpha=0.8)\n",
        "  std_tpr=np.std(tprs,axis=0)\n",
        "  tprs_upper=np.minimum(mean_tpr+std_tpr,1)\n",
        "  tprs_lower=np.maximum(mean_tpr-std_tpr,0)\n",
        "  plt.fill_between(mean_fpr,tprs_lower,tprs_upper,color='grey',alpha=0.2,label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "  plt.plot([0,1],[0,1],color='navy',alpha=0.8,lw=2,linestyle='--')\n",
        "  plt.xlim([0.0,1.0])\n",
        "  plt.ylim([0.0,1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKd4fy0RBDr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('train_data.csv')\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzDoPtY-BlsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets=data.iloc[:,1].values\n",
        "labels=data.iloc[:,0].values\n",
        "print(tweets.shape,labels.shape)\n",
        "print(tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuHoVkO0hqSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  tweet=tweets[i]\n",
        "  r=re.findall(\"@[\\w]* \",tweet)\n",
        "  for j in r:\n",
        "    tweets[i]=re.sub(j,'',tweet)\n",
        "print(tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzzPwqVYB6wJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  tweets[i]=tweets[i].lower()\n",
        "  tweets[i]=''.join([c for c in tweets[i] if c not in punctuation])\n",
        "print(tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffkY1TD1zzrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=' '.join(tweets)\n",
        "words=text.split()\n",
        "counts_words=Counter(words)\n",
        "n_words=len(words)\n",
        "x_sorted_words=counts_words.most_common(n_words)\n",
        "print(counts_words)\n",
        "\n",
        "text=' '.join(labels)\n",
        "words=text.split()\n",
        "counts_words=Counter(words)\n",
        "n_words=len(words)\n",
        "y_sorted_words=counts_words.most_common(n_words)\n",
        "print(counts_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setgqfhr0420",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_vocab={w:i+1 for i,(w,c) in enumerate(x_sorted_words)}\n",
        "tweets_vocab['<unk>']=len(tweets_vocab)\n",
        "print(tweets_vocab)\n",
        "labels_vocab={w:i for i,(w,c) in enumerate(y_sorted_words)}\n",
        "print(labels_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-XF_7ZF1l5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_tokenise=[]\n",
        "for tweet in tweets:\n",
        "    t=[]\n",
        "    for word in tweet.split():\n",
        "      t.append(tweets_vocab[word])\n",
        "    tweets_tokenise.append(t)\n",
        "print(tweets_tokenise[0:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI5ymM4S7tfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_tokenise=[]\n",
        "for i in range(len(labels)):\n",
        "  labels_tokenise.append(labels_vocab[labels[i]])\n",
        "labels_tokenise=np.array(labels_tokenise)\n",
        "print(labels_tokenise)\n",
        "print(labels_tokenise.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TvOfLN0-Qzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_len=[len(tweet) for tweet in tweets_tokenise]\n",
        "max_len=max(tweets_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeHoAKhutMB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_tokenise=pad_tweets(tweets_tokenise,max_len)\n",
        "print(tweets_tokenise)\n",
        "print(tweets_tokenise.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmlTamTMuR7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_split=0.8\n",
        "x_train=tweets_tokenise[0:int(train_test_split*tweets_tokenise.shape[0])]\n",
        "y_train=labels_tokenise[0:int(train_test_split*labels_tokenise.shape[0])]\n",
        "\n",
        "x_val=tweets_tokenise[int(train_test_split*tweets_tokenise.shape[0]):]\n",
        "y_val=labels_tokenise[int(train_test_split*labels_tokenise.shape[0]):]\n",
        "\n",
        "print(x_train.shape,y_train.shape,x_val.shape,y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IJJeKqh04IS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df=TensorDataset(torch.from_numpy(x_train).long().cuda(),torch.from_numpy(y_train).long().cuda())\n",
        "val_df=TensorDataset(torch.from_numpy(x_val).long().cuda(),torch.from_numpy(y_val).long().cuda())\n",
        "\n",
        "batch_size=50\n",
        "train_loader=DataLoader(train_df,shuffle=True,batch_size=batch_size)\n",
        "val_loader=DataLoader(val_df,shuffle=True,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHHGOuvx9rN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,encoder_hidden_dim,decoder_hidden_dim):\n",
        "    super(Attention,self).__init__()\n",
        "    self.encoder_hidden_dim=encoder_hidden_dim\n",
        "    self.decoder_hidden_dim=decoder_hidden_dim\n",
        "    self.attn=nn.Linear((encoder_hidden_dim*2)+decoder_hidden_dim,decoder_hidden_dim)\n",
        "    self.v=nn.Linear(decoder_hidden_dim,1,bias=False)\n",
        "  \n",
        "  def forward(self,encoder_outputs,hidden):\n",
        "    hidden=hidden.unsqueeze(1).repeat(1,max_len,1)\n",
        "    x=torch.cat((hidden,encoder_outputs),dim=2)\n",
        "    x=self.attn(x)\n",
        "    x=torch.tanh(x)\n",
        "    attention=self.v(x).squeeze(2)\n",
        "    attention=F.softmax(attention,dim=1)\n",
        "    return attention\n",
        "\n",
        "class RNNSentimentAnalysis(nn.Module):\n",
        "  def __init__(self,batch_size,output_dim,encoder_hidden_dim,decoder_hidden_dim,vocab_size,embedding_dim,dropout_p):\n",
        "    super(RNNSentimentAnalysis,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.output_dim=output_dim\n",
        "    self.encoder_hidden_dim=encoder_hidden_dim\n",
        "    self.decoder_hidden_dim=decoder_hidden_dim\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.dropout_p=dropout_p\n",
        "\n",
        "    self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
        "    self.enc_rnn=nn.GRU(embedding_dim,encoder_hidden_dim,bidirectional=True)\n",
        "    self.enc_fc=nn.Linear(encoder_hidden_dim*2,decoder_hidden_dim)\n",
        "    self.attention=Attention(encoder_hidden_dim,decoder_hidden_dim)\n",
        "    self.dec_rnn=nn.GRU(encoder_hidden_dim*2,decoder_hidden_dim)\n",
        "    self.dropout=nn.Dropout(self.dropout_p)\n",
        "    self.dec_fc=nn.Linear((encoder_hidden_dim*2)+decoder_hidden_dim,output_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    #encoder\n",
        "    embed=self.embedding(x)\n",
        "    embed=embed.permute(1,0,2)\n",
        "    enc_output,enc_hidden=self.enc_rnn(embed)\n",
        "    enc_hidden=torch.cat((enc_hidden[0,:,:],enc_hidden[1,:,:]),dim=1)\n",
        "    enc_hidden=self.enc_fc(enc_hidden)\n",
        "    enc_hidden=torch.tanh(enc_hidden)\n",
        "    \n",
        "    #attention\n",
        "    enc_output=enc_output.permute(1,0,2)\n",
        "    attn=self.attention(enc_output,enc_hidden)\n",
        "    \n",
        "    #decoder\n",
        "    attn=attn.unsqueeze(1)\n",
        "    weighted_scores=torch.bmm(attn,enc_output)\n",
        "    weighted_scores=weighted_scores.permute(1,0,2)\n",
        "    dec_output,dec_hidden=self.dec_rnn(weighted_scores,enc_hidden.unsqueeze(0))\n",
        "    dec_output=dec_output.squeeze(0)\n",
        "    weighted_scores=weighted_scores.squeeze(0)\n",
        "    x=torch.cat((dec_output,weighted_scores),dim=1)\n",
        "    x=self.dropout(x)\n",
        "    x=self.dec_fc(x)\n",
        "    x=F.softmax(x,dim=1)\n",
        "    return x\n",
        "\n",
        "class LSTMSentimentAnalysis(nn.Module):\n",
        "  def __init__(self,batch_size,output_dim,encoder_hidden_dim,decoder_hidden_dim,vocab_size,embedding_dim,dropout_p):\n",
        "    super(LSTMSentimentAnalysis,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.output_dim=output_dim\n",
        "    self.encoder_hidden_dim=encoder_hidden_dim\n",
        "    self.decoder_hidden_dim=decoder_hidden_dim\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.dropout_p=dropout_p\n",
        "    \n",
        "    self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
        "    self.enc_lstm=nn.LSTM(embedding_dim,encoder_hidden_dim,bidirectional=True)\n",
        "    self.enc_dropout=nn.Dropout(self.dropout_p)\n",
        "    self.enc_fc=nn.Linear(encoder_hidden_dim*2,decoder_hidden_dim)\n",
        "    self.attention=Attention(encoder_hidden_dim,decoder_hidden_dim)\n",
        "    self.dec_lstm=nn.LSTM(encoder_hidden_dim*2,decoder_hidden_dim)\n",
        "    self.dec_dropout=nn.Dropout(self.dropout_p)\n",
        "    self.dec_fc=nn.Linear((encoder_hidden_dim*2)+decoder_hidden_dim,output_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    #encoder\n",
        "    embed=self.embedding(x)\n",
        "    embed=embed.permute(1,0,2)\n",
        "    enc_output,(enc_hidden,enc_cell_state)=self.enc_lstm(embed)\n",
        "    enc_hidden=torch.cat((enc_hidden[0,:,:],enc_hidden[1,:,:]),dim=1)\n",
        "    enc_hidden=self.enc_dropout(enc_hidden)\n",
        "    enc_hidden=self.enc_fc(enc_hidden)\n",
        "    enc_hidden=torch.tanh(enc_hidden)\n",
        "\n",
        "    #attention\n",
        "    enc_output=enc_output.permute(1,0,2)\n",
        "    attn=self.attention(enc_output,enc_hidden)\n",
        "\n",
        "    #decoder\n",
        "    attn=attn.unsqueeze(1)\n",
        "    weighted_scores=torch.bmm(attn,enc_output)\n",
        "    weighted_scores=weighted_scores.permute(1,0,2)\n",
        "    dec_output,(dec_hidden,dec_cell_state)=self.dec_lstm(weighted_scores,(enc_hidden.unsqueeze(0),torch.zeros_like(enc_cell_state[0,:,:].unsqueeze(0))))\n",
        "    dec_output=dec_output.squeeze(0)\n",
        "    weighted_scores=weighted_scores.squeeze(0)\n",
        "    x=torch.cat((dec_output,weighted_scores),dim=1)\n",
        "    x=self.dec_dropout(x)\n",
        "    x=self.dec_fc(x)\n",
        "    x=F.softmax(x,dim=1)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GIJfEmKt5Li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim=len(tweets_vocab)+1\n",
        "output_dim=len(labels_vocab)\n",
        "embedding_dim=50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRc4KYrFA1JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('glove.twitter.27B.50d.pkl','rb') as f:\n",
        "  embedding_dict=pickle.load(f)\n",
        "\n",
        "embedding_weights=np.zeros((input_dim,embedding_dim))\n",
        "words_found=0\n",
        "for i,word in enumerate(tweets_vocab):\n",
        "  try: \n",
        "    embedding_weights[i]=embedding_dict[word]\n",
        "    words_found+=1\n",
        "  except KeyError:\n",
        "    embedding_weights[i]=np.random.normal(scale=0.6,size=(embedding_dim,))\n",
        "embedding_weights=torch.from_numpy(embedding_weights).to(device)\n",
        "\n",
        "# model=RNNSentimentAnalysis(batch_size,output_dim,64,64,input_dim,embedding_dim,dropout_p=0.8)\n",
        "model=LSTMSentimentAnalysis(batch_size,output_dim,128,128,input_dim,embedding_dim,dropout_p=0.8)\n",
        "model.embedding.load_state_dict({'weight':embedding_weights})\n",
        "print(model)\n",
        "model.cuda()\n",
        "total_params=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('total_params:',total_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO-HddO4EOPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimiser=torch.optim.Adam(model.parameters())\n",
        "criterion=nn.BCELoss()\n",
        "epochs=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2mdOV_KCApX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_list=[]\n",
        "train_acc_list=[]\n",
        "val_loss_list=[]\n",
        "val_acc_list=[]\n",
        "for epoch in range(epochs):\n",
        "  train_iterator=iter(train_loader)\n",
        "  val_iterator=iter(val_loader)\n",
        "  train_loss,train_acc=train(model,train_iterator,optimiser,criterion)\n",
        "  val_loss,val_acc=evaluate(model,val_iterator,criterion)\n",
        "\n",
        "  train_loss_list.append(train_loss)\n",
        "  train_acc_list.append(train_acc)\n",
        "  val_loss_list.append(val_loss)\n",
        "  val_acc_list.append(val_acc)\n",
        "  print('Epoch ',epoch+1,'/',epochs,' loss:',train_loss,' acc:',train_acc,' val_loss:',val_loss,' val_acc:',val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoAw2hrI-Lvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.title('loss vs epochs')\n",
        "plt.plot(train_loss_list,label='train')\n",
        "plt.plot(val_loss_list,label='val')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.figure()\n",
        "plt.title('acc vs epochs')\n",
        "plt.plot(train_acc_list,label='train')\n",
        "plt.plot(val_acc_list,label='val')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('acc')\n",
        "plt.legend(loc='upper left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgrBFDOtjHHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if isinstance(model,RNNSentimentAnalysis):\n",
        "  torch.save(model.state_dict(),'/content/drive/My Drive/DL/assgn2/rnn.pt')\n",
        "else:\n",
        "  torch.save(model.state_dict(),'/content/drive/My Drive/DL/assgn2/lstm.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qee6jSDkjybT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model=RNNSentimentAnalysis(batch_size,output_dim,64,64,input_dim,embedding_dim,dropout_p=0.8)\n",
        "model=LSTMSentimentAnalysis(batch_size,output_dim,128,128,input_dim,embedding_dim,dropout_p=0.8)\n",
        "model.cuda()\n",
        "if isinstance(model,RNNSentimentAnalysis):\n",
        "  model.load_state_dict(torch.load('/content/drive/My Drive/DL/assgn2/rnn.pt'))\n",
        "else:\n",
        "  model.load_state_dict(torch.load('/content/drive/My Drive/DL/assgn2/lstm.pt'))\n",
        "\n",
        "train_preds=[]\n",
        "train_gt=[]\n",
        "train_iterator=iter(train_loader)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i,batch in enumerate(train_iterator):\n",
        "    x=batch[0]\n",
        "    y=batch[1]\n",
        "    pred=model(x)\n",
        "    for j in range(pred.size(0)):\n",
        "      pred_i=torch.argmax(pred[j]).cpu().detach().numpy()\n",
        "      train_preds.append(pred_i)\n",
        "      gt_i=y[j].cpu().detach().numpy()\n",
        "      train_gt.append(gt_i)\n",
        "train_preds=np.array(train_preds)\n",
        "train_gt=np.array(train_gt)\n",
        "count=0\n",
        "for i in range(len(train_preds)):\n",
        "  if train_preds[i]==train_gt[i]:\n",
        "    count+=1\n",
        "print(count/len(train_preds))\n",
        "train_cm=confusion_matrix(train_gt,train_preds)\n",
        "print('Train Confusion Matrix:')\n",
        "print(train_cm)\n",
        "roc(train_gt,train_preds)\n",
        "print()\n",
        "\n",
        "val_preds=[]\n",
        "val_gt=[]\n",
        "val_iterator=iter(val_loader)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i,batch in enumerate(val_iterator):\n",
        "    x=batch[0]\n",
        "    y=batch[1]\n",
        "    pred=model(x)\n",
        "    for j in range(pred.size(0)):\n",
        "      pred_i=torch.argmax(pred[j]).cpu().detach().numpy()\n",
        "      val_preds.append(pred_i)\n",
        "      gt_i=y[j].cpu().detach().numpy()\n",
        "      val_gt.append(gt_i)\n",
        "val_preds=np.array(val_preds)\n",
        "val_gt=np.array(val_gt)\n",
        "count=0\n",
        "for i in range(len(val_preds)):\n",
        "  if val_preds[i]==val_gt[i]:\n",
        "    count+=1\n",
        "print(count/len(val_preds))\n",
        "val_cm=confusion_matrix(val_gt,val_preds)\n",
        "print('Validation Confusion Matrix:')\n",
        "print(val_cm)\n",
        "roc(val_gt,val_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTdjnJIEscC6",
        "colab_type": "text"
      },
      "source": [
        "Test Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyOJ-sursfHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data=pd.read_csv('test_data.csv')\n",
        "test_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB4KO-_Gsii0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets=test_data.iloc[:,1].values\n",
        "labels=test_data.iloc[:,0].values\n",
        "print(tweets.shape,labels.shape)\n",
        "print(tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X_mV5sisrnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  tweet=tweets[i]\n",
        "  r=re.findall(\"@[\\w]* \",tweet)\n",
        "  for j in r:\n",
        "    tweets[i]=re.sub(j,'',tweet)\n",
        "print(tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xigMhqEUZWeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  tweets[i]=tweets[i].lower()\n",
        "  tweets[i]=''.join([c for c in tweets[i] if c not in punctuation])\n",
        "print(tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHpNZ7kos62a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_tokenise=[]\n",
        "for tweet in tweets:\n",
        "  t=[]\n",
        "  for word in tweet.split():\n",
        "    try:\n",
        "      t.append(tweets_vocab[word])\n",
        "    except KeyError:\n",
        "      t.append(len(tweets_vocab))\n",
        "  tweets_tokenise.append(t)\n",
        "print(tweets_tokenise[0:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKR8iV3eat5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_tokenise=[]\n",
        "for i in range(len(labels)):\n",
        "  labels_tokenise.append(labels_vocab[labels[i]])\n",
        "labels_tokenise=np.array(labels_tokenise)\n",
        "print(labels_tokenise)\n",
        "print(labels_tokenise.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEPY0jlqtKQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_tokenise=pad_tweets(tweets_tokenise,max_len)\n",
        "print(tweets_tokenise)\n",
        "print(tweets_tokenise.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP-EW8smtVqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test=tweets_tokenise[:]\n",
        "y_test=labels_tokenise[:]\n",
        "\n",
        "print(x_test.shape,y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIqqFr1hti3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df=TensorDataset(torch.from_numpy(x_test).long().cuda(),torch.from_numpy(y_test).long().cuda())\n",
        "batch_size=50\n",
        "test_loader=DataLoader(test_df,shuffle=True,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG9Zk-DEuEBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model=RNNSentimentAnalysis(batch_size,13,64,64,input_dim,embedding_dim,0.8)\n",
        "model=LSTMSentimentAnalysis(batch_size,13,128,128,input_dim,embedding_dim,0.8)\n",
        "model.cuda()\n",
        "if isinstance(model,RNNSentimentAnalysis):\n",
        "  model.load_state_dict(torch.load('/content/drive/My Drive/DL/assgn2/rnn.pt'))\n",
        "else:\n",
        "  model.load_state_dict(torch.load('/content/drive/My Drive/DL/assgn2/lstm.pt'))\n",
        "\n",
        "test_preds=[]\n",
        "test_gt=[]\n",
        "test_iterator=iter(test_loader)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i,batch in enumerate(test_iterator):\n",
        "    x=batch[0]\n",
        "    y=batch[1]\n",
        "    pred=model(x)\n",
        "    y=y.cpu().detach().numpy()\n",
        "    y=np.eye(13)[y]\n",
        "    y=torch.from_numpy(y).float().cuda()\n",
        "    for j in range(pred.size(0)):\n",
        "      test_preds.append(pred[j].cpu().detach().numpy())\n",
        "      test_gt.append(y[j].cpu().detach().numpy())\n",
        "test_preds=np.array(test_preds)\n",
        "test_gt=np.array(test_gt)\n",
        "test_acc=accuracy(torch.from_numpy(test_preds).float().cuda(),torch.from_numpy(test_gt).float().cuda())\n",
        "print('Accuracy:',test_acc*100,'%')\n",
        "\n",
        "preds=[]\n",
        "gt=[]\n",
        "for i in range(test_preds.shape[0]):\n",
        "  pred_i=test_preds[i]\n",
        "  gt_i=test_gt[i]\n",
        "  preds.append(np.argmax(pred_i))\n",
        "  gt.append(np.argmax(gt_i))\n",
        "\n",
        "gt=np.array(gt)\n",
        "preds=np.array(preds)\n",
        "test_cm=confusion_matrix(gt,preds)\n",
        "print('Test Confusion Matrix:')\n",
        "print(test_cm)\n",
        "roc(gt,preds)\n",
        "\n",
        "test_pred_labels=[]\n",
        "id_arr=np.zeros((preds.shape[0]),dtype=np.int64)\n",
        "for i in range(preds.shape[0]):\n",
        "  for k,v in labels_vocab.items():\n",
        "    if v==preds[i]:\n",
        "      test_pred_labels.append(k)\n",
        "  id_arr[i]=i+1\n",
        "\n",
        "test_pred_labels=np.array(test_pred_labels)\n",
        "out_arr=np.column_stack((id_arr,test_pred_labels))\n",
        "out_df=pd.DataFrame(data=out_arr,columns=['ID','Class'])\n",
        "if isinstance(model,RNNSentimentAnalysis):\n",
        "  out_df.to_csv('test_results_rnn.csv',index=False)\n",
        "else:\n",
        "  out_df.to_csv('test_results_lstm.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}